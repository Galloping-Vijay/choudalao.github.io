---
title: "Spider爬取臭大佬首页文章"
date: 2019-12-25T22:44:50+08:00
updated: 2026-02-23T12:06:40+08:00
author: "臭大佬"
categories: [Python]
description: "Spider爬取臭大佬首页文章"
cover: "https://www.choudalao.com/uploads/20191225/dzapGZvLB5jDvfrQb8gcMHvmqFCtSuGKjKwxNgH7.jpeg"
click: 3342
---

# 创建Scrapy项目

```shell
scrapy startproject choudalao
```

> λ scrapy startproject choudalao
New Scrapy project 'choudalao', using template directory 'd:\program files\python\lib\site-packages\scrapy\templates\project', created in:
    D:\wwwroot\dev.py.net\choudalao

>You can start your first spider with:
    cd choudalao
    scrapy genspider example example.com
	
![](https://www.choudalao.com/uploads/20191225/20191225204906M9bMwy.png)

目录结构：

![](https://www.choudalao.com/uploads/20191225/20191225204945vrkFSJ.png)

# 创建爬虫

进入spider目录下，运行命令
 ```python
scrapy genspider choudalao "choudalao.com"
```
![](https://www.choudalao.com/uploads/20191225/20191225210234zSUgiP.png)


# 分析
打开臭大佬[首页](https://www.choudalao.com "首页")，分析数据结构:
![](https://www.choudalao.com/uploads/20191225/20191225205632lCwMqZ.png)

![](https://www.choudalao.com/uploads/20191225/20191225205833STF6pV.png)
每一个li里面都包含标题、作者、链接、标签、浏览数、时间等等。我们就提取这几个字段。

#利用xPath提取内容

```python
//div[@class="blogs"]/ul/li
```
![](https://www.choudalao.com/uploads/20191225/20191225212320Q4F3Ia.png)

提取所有标题：
![](https://www.choudalao.com/uploads/20191225/20191225213512KLBIoF.png)
别的以此类推

# 设置字段
打开 `items.py`，编辑如下：
```python
class ChoudalaoItem(scrapy.Item):
    # 文章标题
    title = scrapy.Field()
    # 作者
    author = scrapy.Field()
    # 链接
    link = scrapy.Field()
    # 标签
    keyword = scrapy.Field()
    # 浏览数
    looks = scrapy.Field()
    # 时间
    craete_time = scrapy.Field()

```

# 提取数据

```python
# -*- coding: utf-8 -*-
import scrapy

from choudalao.items import ChoudalaoItem


class ChoudalaoSpider(scrapy.Spider):
    name = 'choudalao'
    allowed_domains = ['www.choudalao.com']
    # 起始地址(带分页)
    baseUrl = "https://www.choudalao.com/?page="
    page = 1
    start_urls = [baseUrl + str(page)]

    def parse(self, response):
        # 接收数据
        node_list = response.xpath('//div[@class="blogs"]/ul/li')
        # 遍历数据
        for node in node_list:
            # 存储到每个item
            item = ChoudalaoItem()
            # 标题
            item['title'] = node.xpath('./h3/a/text()').extract()[0]
            # 标签
            item['keyword'] = node.xpath('./div[@class="autor"]/span[@class="lm"]/a/text()').extract()[0]
            # 作者
            item['author'] = node.xpath('./div[@class="autor"]/span[@class="author"]/text()').extract()[0]
            # 链接
            item['link'] = node.xpath('./span[@class="blogpic"]/a/@href').extract()[0]
            # 浏览数
            item['looks'] = node.xpath('./div[@class="autor"]/span[@class="viewnum"]/a/text()').extract()[0]
            # 时间
            item['craete_time'] = node.xpath('./div[@class="autor"]/span[@class="dtime"]/text()').extract()[0]

            yield item

```

# 编写管道文件

```python
# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html
import json


class ChoudalaoPipeline(object):
    def __init__(self):
        self.f = open("choudalao.json", "w", encoding='utf-8')

    def process_item(self, item, spider):
        # 转成字典,写入json格式
        content = json.dumps(dict(item), ensure_ascii=False) + ",\n"
        self.f.write(content)
        return item

    def clone_spider(self, spider):
        self.f.close()

```

# 配置启用管道
在settings.py文件中搜索`ITEM_PIPELINES`，打开注释
```python
ITEM_PIPELINES = {
   'choudalao.pipelines.ChoudalaoPipeline': 300,
}
```
![](https://www.choudalao.com/uploads/20191225/201912252206399PVMGC.png)

# 运行

```python
scrapy crawl choudalao
```
![](https://www.choudalao.com/uploads/20191226/20191226000835C39YY9.png)

抓取的数据截图
![](https://www.choudalao.com/uploads/20191226/20191226001315NB2C1u.png)